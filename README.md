# 摘要
> 学习器模型中一般有两种参数，一种参数是可以从学习中得到，还有一种无法靠数据里面得到，只能靠人的经验来设定，这类参数就叫做超参数。本文讨论如何进行超参数的选择

---

# 什么是超参数
> 超参数是在开始学习过程之前设置值的参数。 相反，其他参数的值通过训练得出。
- 定义关于模型的更高层次的概念，如复杂性或学习能力。
- 不能直接从标准模型培训过程中的数据中学习，需要预先定义。
- 可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定

# 有哪些超参数

- 学习率 η
- 正则化参数 λ
- 神经网络的层数 L
- 每一个隐层中神经元的个数 j
- 学习的回合数Epoch
- 小批量数据 minibatch 的大小
- 输出神经元的编码方式
- 代价函数的选择
- 权重初始化的方法
- 神经元激活函数的种类
- 参加训练模型数据的规模
- 学习率调整方法 

# 超参数调节方法
- 网格搜索
> 指定参数值的一种穷举搜索方法，通过将估计函数的参数通过交叉验证的方法进行优化来得到最优的学习算法。 
即，将各个参数可能的取值进行排列组合，列出所有可能的组合结果生成“网格”。然后将各组合用于SVM训练，并使用交叉验证对表现进行评估。在拟合函数尝试了所有的参数组合后，返回一个合适的分类器，自动调整至最佳参数组合
一种手动指定一组超参数的穷举搜索法



- 随机搜索
> 在高维空间上并不起作用，因为它太容易遇到维度灾难了。而对于随机搜索来说，进行稀疏的简单随机抽样并不会遇到该问题，因此随机搜索方法广泛地应用于实践中
由粗到细


- 贝叶斯优化
> 随机搜索并不能利用先验知识来选择下一组超参数，这一缺点在训练成本较高的模型中尤为突出。
使用一些已知的先验知识逼近或猜测该函数是什么。通过先验知识进行最优点的预测。假设相似的输入会产生相似的输出，符合多变量高斯分布


- 基于梯度的优化
- [遗传算法的实践](http://www.sohu.com/a/207091529_465975)



# 单个超参数调节
# 多个超参数调节
## 自然数坐标
## 指数坐标
# 演示代码
- []


# 结论
> 使用随机搜索（不要用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。


---
参考资料
- [百度百科](https://baike.baidu.com/item/%E8%B6%85%E5%8F%82%E6%95%B0/3101858?fr=aladdin)
- [超参数的选择与交叉验证](https://www.cnblogs.com/liuyu124/p/7332594.html)
- [DrMAD：深度神经网络中的超参数学习器](https://zhuanlan.zhihu.com/p/26806596?utm_source=tuicool&utm_medium=referral)
- [十、如何选择神经网络的超参数](http://blog.csdn.net/dugudaibo/article/details/77366245)
- [知乎-深度学习调参有哪些技巧？](https://www.zhihu.com/question/25097993)
- [如何选取一个神经网络中的超参数hyper-parameters](http://www.mamicode.com/info-detail-1851850.html)
- [如何调试神经网络（深度神经网络）](http://cvmart.net/community/article/detail/36)
- [稍微深入地介绍贝叶斯优化](http://blog.csdn.net/cqzz513524327/article/details/72772205)
- [机器学习算法上的贝叶斯优化实践](https://zhuanlan.zhihu.com/p/23346674)
- [教程 | 拟合目标函数后验分布的调参利器：贝叶斯优化](http://www.sohu.com/a/165565029_465975)
- [Hype：组合机器学习和超参数优化](http://www.iteye.com/news/31552)